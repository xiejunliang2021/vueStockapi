# -*- coding: UTF-8 -*-
'''
@Project ï¼švueStockapi 
@File ï¼štest.py
@Author ï¼šAnita_ç†™çƒ¨ï¼ˆè·¯è™½è¿œï¼Œè¡Œåˆ™é™è‡³ï¼äº‹è™½éš¾ï¼Œåšåˆ™å¿…æˆï¼ï¼‰
@Date ï¼š2026/1/8 20:00 
@JianShu : 
'''
import logging
import urllib.parse
import re
import base64
import os
import datetime
import asyncio
import pandas as pd
import json
import uuid
import subprocess
from telegram import Update
from telegram.ext import ApplicationBuilder, ContextTypes, MessageHandler, filters, CommandHandler

# --- åŸºç¡€é…ç½® ---
BOT_TOKEN = '8436411560:AAENdrBrn25ZjR3KS3WBrqjVkDNtLFaofb0'
STATIC_FILE_PATH = "/var/www/html/subdata/nodes.txt"
IP_POOL_CSV = "/var/www/html/subdata/ip_pool.csv"
NODE_TEMPLATES_CSV = "/var/www/html/subdata/node_templates.csv"
CONFIG_FILE = "/var/www/html/subdata/config.json"
SUB_BASE_URL = "https://subapi.832693.xyz/subdata/nodes.txt"

# Nginx é…ç½®æ–‡ä»¶è·¯å¾„
NGINX_CONF_PATH = "/etc/nginx/conf.d/subconverter.conf"

# é»˜è®¤é…ç½®
DEFAULT_CONFIG = {
    "latency_limit": 200.0,
    "sub_token": str(uuid.uuid4())
}

# æ­£åˆ™è¡¨è¾¾å¼
ADDR_PATTERN = re.compile(
    r'([a-zA-Z0-9][-a-zA-Z0-9]{0,62}(?:\.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})+|[0-9]{1,3}(?:\.[0-9]{1,3}){3})')
MS_PATTERN = re.compile(r'(\d+(?:\.\d+)?)\s*ms')
IP_ONLY_PATTERN = r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$'

logging.basicConfig(level=logging.INFO)


# --- å·¥å…·å‡½æ•° ---

def load_config():
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r') as f:
                config = json.load(f)
                if "sub_token" not in config:
                    config["sub_token"] = DEFAULT_CONFIG["sub_token"]
                return config
        except:
            return DEFAULT_CONFIG
    return DEFAULT_CONFIG


def save_config(config):
    try:
        with open(CONFIG_FILE, 'w') as f:
            json.dump(config, f)
    except Exception as e:
        print(f"Save Config Error: {e}")


def update_nginx_config(new_token):
    print(f"å¼€å§‹æ›´æ–° Nginx é…ç½®: {NGINX_CONF_PATH}")
    try:
        if not os.path.exists(NGINX_CONF_PATH):
            return False, f"æœªæ‰¾åˆ°æ–‡ä»¶: {NGINX_CONF_PATH}"

        with open(NGINX_CONF_PATH, 'r', encoding='utf-8') as f:
            content = f.read()

        pattern = r'\$arg_token\s*!=\s*"[^"]+"'
        if not re.search(pattern, content):
            return False, "Nginx é…ç½®ä¸­æœªåŒ¹é…åˆ° $arg_token æ ¡éªŒè¡Œ"

        new_content = re.sub(pattern, f'$arg_token != "{new_token}"', content)
        with open(NGINX_CONF_PATH, 'w', encoding='utf-8') as f:
            f.write(new_content)

        subprocess.run(["sudo", "nginx", "-t"], check=True, capture_output=True)
        subprocess.run(["sudo", "nginx", "-s", "reload"], check=True, capture_output=True)
        return True, "Nginx é…ç½®åŒæ­¥æˆåŠŸ"
    except Exception as e:
        return False, f"å¼‚å¸¸: {str(e)}"


def init_csv_files():
    os.makedirs(os.path.dirname(IP_POOL_CSV), exist_ok=True)
    for path, cols in [(IP_POOL_CSV, ['address', 'latency', 'last_checked']),
                       (NODE_TEMPLATES_CSV, ['node_url', 'remarks'])]:
        if not os.path.exists(path) or os.path.getsize(path) == 0:
            pd.DataFrame(columns=cols).to_csv(path, index=False)


async def check_tcp_latency(ip, port=443, timeout=2):
    start = asyncio.get_event_loop().time()
    try:
        conn = asyncio.open_connection(ip, port)
        _, writer = await asyncio.wait_for(conn, timeout=timeout)
        writer.close()
        await writer.wait_closed()
        return int((asyncio.get_event_loop().time() - start) * 1000)
    except:
        return None


# --- æ ¸å¿ƒé€»è¾‘ ---

async def generate_subscription():
    config = load_config()
    limit = config.get("latency_limit", 200.0)
    try:
        ips_df = pd.read_csv(IP_POOL_CSV)
        templates_df = pd.read_csv(NODE_TEMPLATES_CSV)
        if ips_df.empty or templates_df.empty: return None, 0

        valid_nodes = []
        tasks = [check_tcp_latency(row['address']) for _, row in ips_df.iterrows()]
        results = await asyncio.gather(*tasks)

        for _, t_row in templates_df.iterrows():
            try:
                parsed = urllib.parse.urlparse(t_row['node_url'])
                user_info, server_info = parsed.netloc.split('@')
                port = server_info.split(':')[1] if ':' in server_info else ""
                tag_base = urllib.parse.unquote(parsed.fragment) if parsed.fragment else "Node"

                for addr, ms in zip(ips_df['address'], results):
                    if ms is None or ms >= limit: continue
                    new_loc = f"{user_info}@{addr}:{port}" if port else f"{user_info}@{addr}"
                    new_node = urllib.parse.urlunparse(
                        parsed._replace(netloc=new_loc, fragment=urllib.parse.quote(f"{tag_base}-{ms}ms")))
                    valid_nodes.append((new_node, ms))
            except:
                continue

        valid_nodes.sort(key=lambda x: x[1])
        final = [x[0] for x in valid_nodes[:100]]
        if final:
            b64 = base64.b64encode("\n".join(final).encode()).decode()
            with open(STATIC_FILE_PATH, "w") as f: f.write(b64)
            return b64, len(final)
    except Exception as e:
        print(f"Generate Error: {e}")
    return None, 0


async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    å¤„ç†éå‘½ä»¤æ–‡æœ¬æ¶ˆæ¯
    è‡ªåŠ¨è¯†åˆ«IPæˆ–åŸŸåå¹¶åˆ†ç±»å¤„ç†
    """
    if not update.message or not update.message.text:
        return
    if update.message.text.startswith('/'):
        return

    text = update.message.text
    line_count = len([l for l in text.split('\n') if l.strip()])

    # åªå¤„ç†æ˜æ˜¾çš„æ‰¹é‡å¯¼å…¥
    if line_count < 3:
        return

    # åˆ†åˆ«ç»Ÿè®¡IPå’ŒåŸŸå
    ip_count = await process_ip_logic(text)
    domain_count = await process_domain_logic(text)

    if ip_count > 0 or domain_count > 0:
        msg = " è‡ªåŠ¨è¯†åˆ«å®Œæˆ\n"
        if ip_count > 0:
            msg += f"- IP: {ip_count} ä¸ª\n"
        if domain_count > 0:
            msg += f"- åŸŸå: {domain_count} ä¸ª"
        await update.message.reply_text(msg)
    else:
        await update.message.reply_text(" æœªè¯†åˆ«åˆ°æœ‰æ•ˆåœ°å€,è¯·ä½¿ç”¨å‘½ä»¤æ‰‹åŠ¨æ·»åŠ ")


# --- æ–°å¢: æŸ¥çœ‹åŸŸååˆ—è¡¨ ---
async def list_domains_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """åªæ˜¾ç¤ºåŸŸå(è¿‡æ»¤æ‰IP)"""
    try:
        df = pd.read_csv(IP_POOL_CSV)
        # è¿‡æ»¤å‡ºåŸŸå(éçº¯IPæ ¼å¼)
        domain_df = df[~df['address'].str.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'
                                                , na=False)]

        if domain_df.empty:
            await update.message.reply_text(" æš‚æ— åŸŸåè®°å½•")
            return

        msg = f" åŸŸååˆ—è¡¨ (å…± {len(domain_df)} ä¸ª)\n\n"

        # æ˜¾ç¤ºå‰20ä¸ª,æŒ‰å»¶è¿Ÿæ’åº
        sorted_df = domain_df.sort_values('latency').head(20)
        for _, row in sorted_df.iterrows():
            latency_str = f"{row['latency']:.0f}ms" if row['latency'] > 0 else "æœªæµ‹"
            msg += f"{latency_str}: {row['address']}\n"

        if len(domain_df) > 20:
            msg += f"\n... è¿˜æœ‰ {len(domain_df) - 20} ä¸ªåŸŸå"

        await update.message.reply_text(msg)
    except Exception as e:
        await update.message.reply_text(f" è¯»å–å¤±è´¥: {str(e)}")


# --- æå–ä¸ä¿å­˜ IP/åŸŸå çš„é€šç”¨å‡½æ•° ---

async def process_ip_logic(text):
    """
    åªå¤„ç†çº¯IPåœ°å€
    åŸŸåè¯·ä½¿ç”¨ process_domain_logic
    """
    new_entries = []
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # ä¸¥æ ¼çš„IPæ­£åˆ™
    ip_pattern = re.compile(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b')

    lines = text.replace('\n', ' ').split()

    for item in lines:
        item = item.strip()
        if not item or "://" in item:
            continue

        # æå–å»¶è¿Ÿ
        latency = 0
        ms_match = MS_PATTERN.search(item)
        if ms_match:
            latency = float(ms_match.group(1))
            item = item.replace(ms_match.group(0), "")

        # åªåŒ¹é…IP
        ip_match = ip_pattern.search(item)
        if ip_match:
            ip = ip_match.group(0)
            # éªŒè¯IPæœ‰æ•ˆæ€§
            parts = ip.split('.')
            if all(0 <= int(p) <= 255 for p in parts):
                new_entries.append({
                    'address': ip,
                    'latency': latency,
                    'last_checked': now
                })

    if new_entries:
        df = pd.read_csv(IP_POOL_CSV)
        updated_df = pd.concat([pd.DataFrame(new_entries), df], ignore_index=True)
        updated_df = updated_df.drop_duplicates(subset=['address'], keep='first')
        updated_df.to_csv(IP_POOL_CSV, index=False)
        return len(new_entries)

    return 0


# --- æŒ‡ä»¤å¤„ç†å™¨ ---

async def add_ip_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        # æ”¹è¿›1: ç”¨æ¢è¡Œç¬¦è¿æ¥å‚æ•°,ä¿æŒå¤šè¡Œç»“æ„
        text = "\n".join(context.args).strip()

        if not text:
            await update.message.reply_text("ç”¨æ³•: /add_ip [IPæˆ–åŸŸå]\næ”¯æŒå¤šä¸ª,ç”¨ç©ºæ ¼æˆ–æ¢è¡Œåˆ†éš”")
            return

        count = await process_ip_logic(text)
        if count > 0:
            await update.message.reply_text(f" æˆåŠŸæ·»åŠ  {count} ä¸ªåœ°å€")
        else:
            await update.message.reply_text(" æœªè¯†åˆ«åˆ°æœ‰æ•ˆåœ°å€")
    except Exception as e:
        await update.message.reply_text(f" æ·»åŠ å¤±è´¥: {str(e)}")


async def clear_ips_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """ä»…æ¸…ç©ºçº¯ IPï¼Œä¿ç•™åŸŸå"""
    try:
        df = pd.read_csv(IP_POOL_CSV)
        initial_count = len(df)

        # æ”¹è¿›4: ä½¿ç”¨æ›´ä¸¥æ ¼çš„IPæ­£åˆ™
        ip_pattern = r'^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$'
        df_remaining = df[~df['address'].str.match(ip_pattern, na=False)]

        cleared_count = initial_count - len(df_remaining)
        df_remaining.to_csv(IP_POOL_CSV, index=False)

        await update.message.reply_text(
            f" æ¸…ç†å®Œæˆ\n"
            f"- åˆ é™¤IP: {cleared_count} ä¸ª\n"
            f"- ä¿ç•™åŸŸå: {len(df_remaining)} ä¸ª"
        )
    except Exception as e:
        await update.message.reply_text(f" æ¸…ç©ºå¼‚å¸¸: {str(e)}")


async def get_sub_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    config = load_config()
    token = config.get("sub_token")
    full_url = f"{SUB_BASE_URL}?token={token}"
    await update.message.reply_text(f"è®¢é˜…é“¾æ¥:\n{full_url}")


async def refresh_uuid_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    new_token = str(uuid.uuid4())
    config = load_config()
    config["sub_token"] = new_token
    save_config(config)
    success, message = update_nginx_config(new_token)
    await update.message.reply_text(f"UUID åˆ·æ–°ç»“æœ: {message}\næ–° Token: {new_token}")


async def add_node_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        parts = update.message.text.split(None, 1)
        if len(parts) < 2:
            await update.message.reply_text("ç”¨æ³•: /add_node [é“¾æ¥]")
            return
        url = parts[1].strip()
        now_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
        df = pd.read_csv(NODE_TEMPLATES_CSV)
        new_row = pd.DataFrame([{'node_url': url, 'remarks': now_time}])
        pd.concat([df, new_row], ignore_index=True).drop_duplicates(subset=['node_url']).to_csv(NODE_TEMPLATES_CSV,
                                                                                                index=False)
        await update.message.reply_text(f"æˆåŠŸ: èŠ‚ç‚¹å·²å½•å…¥ ({now_time})")
    except Exception as e:
        await update.message.reply_text(f"å¼‚å¸¸: {str(e)}")


async def process_domain_logic(text):
    """
    ä¸“é—¨å¤„ç†åŸŸåçš„å‡½æ•°
    æ”¯æŒæ ¼å¼:
    - çº¯åŸŸå: example.com
    - å¸¦å»¶è¿Ÿ: 123.45 ms: example.com
    - å¸¦åè®®: https://example.com (è‡ªåŠ¨æå–åŸŸå)
    """
    new_entries = []
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # æ›´å¼ºçš„åŸŸåæ­£åˆ™ (æ”¯æŒå­åŸŸå)
    domain_pattern = re.compile(
        r'(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}'
    )

    # åŒæ—¶æ”¯æŒæ¢è¡Œå’Œç©ºæ ¼åˆ†å‰²
    lines = text.replace('\n', ' ').split()

    for item in lines:
        item = item.strip()
        if not item:
            continue

        # æå–å»¶è¿Ÿ
        latency = 0
        ms_match = MS_PATTERN.search(item)
        if ms_match:
            latency = float(ms_match.group(1))
            item = item.replace(ms_match.group(0), "")

        # ç§»é™¤åè®®å‰ç¼€
        item = re.sub(r'^https?://', '', item)
        item = re.sub(r'^www\.', '', item)

        # ç§»é™¤è·¯å¾„å’Œå‚æ•°
        item = item.split('/')[0].split('?')[0].split('#')[0]

        # æå–åŸŸå
        domain_match = domain_pattern.search(item)
        if domain_match:
            domain = domain_match.group(0).lower()  # ç»Ÿä¸€å°å†™

            # éªŒè¯ä¸æ˜¯çº¯IP
            if not re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'
                    , domain):
                new_entries.append({
                    'address': domain,
                    'latency': latency,
                    'last_checked': now
                })

    # ä¿å­˜åˆ°CSV
    if new_entries:
        df = pd.read_csv(IP_POOL_CSV)
        updated_df = pd.concat([pd.DataFrame(new_entries), df], ignore_index=True)
        # å»é‡: ä¿ç•™æœ€æ–°çš„è®°å½•
        updated_df = updated_df.drop_duplicates(subset=['address'], keep='first')
        updated_df.to_csv(IP_POOL_CSV, index=False)
        return len(new_entries)

    return 0


# --- æ–°å¢: åŸŸåä¸Šä¼ å‘½ä»¤ ---
async def add_domain_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    ä¸“é—¨ä¸Šä¼ åŸŸåçš„å‘½ä»¤
    ç”¨æ³•: /add_domain example.com www.test.com
    æˆ–å¤šè¡Œç²˜è´´åŸŸååˆ—è¡¨
    """
    try:
        text = "\n".join(context.args).strip()

        if not text:
            await update.message.reply_text(
                " ç”¨æ³•: /add_domain [åŸŸå]\n\n"
                "æ”¯æŒæ ¼å¼:\n"
                "- example.com\n"
                "- 123ms: www.test.com\n"
                "- https://cdn.example.org\n\n"
                "æ”¯æŒæ‰¹é‡ç²˜è´´,è‡ªåŠ¨å»é‡"
            )
            return

        count = await process_domain_logic(text)

        if count > 0:
            await update.message.reply_text(
                f" åŸŸåæ·»åŠ æˆåŠŸ\n"
                f"æ–°å¢: {count} ä¸ª\n"
                f"å·²è‡ªåŠ¨å»é‡"
            )
        else:
            await update.message.reply_text(
                " æœªè¯†åˆ«åˆ°æœ‰æ•ˆåŸŸå\n\n"
                "è¯·ç¡®ä¿æ ¼å¼æ­£ç¡®:\n"
                "âœ“ example.com\n"
                "âœ“ sub.domain.org\n"
                "âœ— 192.168.1.1 (è¿™æ˜¯IP)"
            )
    except Exception as e:
        logging.error(f"add_domainé”™è¯¯: {e}", exc_info=True)
        await update.message.reply_text(f" æ·»åŠ å¤±è´¥: {str(e)}")




async def get_commands_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    help_text = (
        "ğŸ“‹ æŒ‡ä»¤åˆ—è¡¨\n\n"
        "ã€èŠ‚ç‚¹ç®¡ç†ã€‘\n"
        " /add_node [URL] - æ·»åŠ èŠ‚ç‚¹æ¨¡æ¿\n"
        " /nodes - æŸ¥çœ‹èŠ‚ç‚¹æ¨¡æ¿\n"
        " /del_node [ç¼–å·] - åˆ é™¤èŠ‚ç‚¹æ¨¡æ¿\n\n"
        "ã€åœ°å€ç®¡ç†ã€‘\n"
        " /add_ip [IP] - æ·»åŠ IPåœ°å€\n"
        " /add_domain [åŸŸå] - æ·»åŠ åŸŸå ğŸ†•\n"
        " /ips - æŸ¥çœ‹IPåˆ—è¡¨\n"
        " /domains - æŸ¥çœ‹åŸŸååˆ—è¡¨ ğŸ†•\n"
        " /del_ip [åœ°å€] - åˆ é™¤æŒ‡å®šåœ°å€\n"
        " /clear_ips - æ¸…ç©ºIP(ä¿ç•™åŸŸå)\n\n"
        "ã€è®¢é˜…ç®¡ç†ã€‘\n"
        " /get_sub - è·å–è®¢é˜…é“¾æ¥\n"
        " /refresh_uuid - åˆ·æ–°Token\n"
        " /refresh - æµ‹é€Ÿæ›´æ–°\n\n"
        " /get - æ˜¾ç¤ºæ­¤åˆ—è¡¨"
    )
    await update.message.reply_text(help_text)


async def refresh_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("æ­£åœ¨æ›´æ–°...")
    _, count = await generate_subscription()
    await update.message.reply_text(f"å®Œæˆï¼Œæœ‰æ•ˆèŠ‚ç‚¹æ•°é‡: {count}")


async def list_ips_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        df = pd.read_csv(IP_POOL_CSV)
        msg = f"æ€»è®¡: {len(df)}\n" + "\n".join(df.head(10)['address'].tolist())
        await update.message.reply_text(msg)
    except:
        await update.message.reply_text("è¯»å–å¤±è´¥")


async def list_nodes_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        df = pd.read_csv(NODE_TEMPLATES_CSV)
        msg = "èŠ‚ç‚¹åˆ—è¡¨:\n"
        for i, row in df.iterrows():
            msg += f"{i + 1}. {row['remarks']} | {row['node_url'][:30]}...\n"
        await update.message.reply_text(msg)
    except:
        await update.message.reply_text("è¯»å–å¤±è´¥")


async def del_node_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        idx = int(context.args[0]) - 1
        df = pd.read_csv(NODE_TEMPLATES_CSV)
        df.drop(df.index[idx]).to_csv(NODE_TEMPLATES_CSV, index=False)
        await update.message.reply_text("åˆ é™¤æˆåŠŸ")
    except:
        await update.message.reply_text("ç¼–å·é”™è¯¯")


async def del_ip_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        target = " ".join(context.args).strip()
        df = pd.read_csv(IP_POOL_CSV)
        df[df['address'] != target].to_csv(IP_POOL_CSV, index=False)
        await update.message.reply_text(f"å·²å¤„ç†: {target}")
    except:
        await update.message.reply_text("æ“ä½œé”™è¯¯")



if __name__ == '__main__':
    init_csv_files()
    app = ApplicationBuilder().token(BOT_TOKEN).build()

    # === å¸®åŠ©å’Œä¿¡æ¯å‘½ä»¤ ===
    app.add_handler(CommandHandler("get", get_commands_handler))
    app.add_handler(CommandHandler("get_sub", get_sub_handler))

    # === è®¢é˜…ç®¡ç†å‘½ä»¤ ===
    app.add_handler(CommandHandler("refresh_uuid", refresh_uuid_handler))
    app.add_handler(CommandHandler("refresh", refresh_handler))

    # === èŠ‚ç‚¹æ¨¡æ¿ç®¡ç† ===
    app.add_handler(CommandHandler(["add_node", "addnode"], add_node_handler))
    app.add_handler(CommandHandler("nodes", list_nodes_handler))
    app.add_handler(CommandHandler(["del_node", "delnode"], del_node_handler))

    # === IP/åŸŸååœ°å€ç®¡ç† ===
    app.add_handler(CommandHandler("add_ip", add_ip_handler))
    app.add_handler(CommandHandler(["add_domain", "adddomain"], add_domain_handler))  # ğŸ†• æ–°å¢åŸŸåå‘½ä»¤
    app.add_handler(CommandHandler("ips", list_ips_handler))
    app.add_handler(CommandHandler(["domains", "domain"], list_domains_handler))  # ğŸ†• æ–°å¢æŸ¥çœ‹åŸŸå
    app.add_handler(CommandHandler("del_ip", del_ip_handler))
    app.add_handler(CommandHandler("clear_ips", clear_ips_handler))

    # === æ–‡æœ¬æ¶ˆæ¯è‡ªåŠ¨è¯†åˆ« ===
    app.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), handle_text))

    # === å¯åŠ¨æç¤º ===
    print("=" * 50)
    print("Telegram Bot å¯åŠ¨æˆåŠŸ!")
    print("=" * 50)
    print(" å¯ç”¨å‘½ä»¤:")
    print("  - /add_ip      : æ·»åŠ IPåœ°å€")
    print("  - /add_domain  : æ·»åŠ åŸŸå (æ–°åŠŸèƒ½)")
    print("  - /ips         : æŸ¥çœ‹IPåˆ—è¡¨")
    print("  - /domains     : æŸ¥çœ‹åŸŸååˆ—è¡¨ (æ–°åŠŸèƒ½)")
    print("  - /add_node    : æ·»åŠ èŠ‚ç‚¹æ¨¡æ¿")
    print("  - /get_sub     : è·å–è®¢é˜…é“¾æ¥")
    print("  - /get         : æŸ¥çœ‹å®Œæ•´å‘½ä»¤åˆ—è¡¨")
    print("=" * 50)
    print(" ç­‰å¾…æ¶ˆæ¯ä¸­...")
    print("=" * 50)

    app.run_polling()